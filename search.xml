<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[史上最好理解的Unicode编码讲解（Unicode的前世今生）]]></title>
    <url>%2F2019%2F07%2F30%2F%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%90%86%E8%A7%A3%E7%9A%84Unicode%E7%BC%96%E7%A0%81%E8%AE%B2%E8%A7%A3%EF%BC%88Unicode%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于Unicode的由来，以及与utf-8、utf-16的关系、原理、计算、编码和解码。用最通俗易懂的方法讲解。 一、了解编码 在计算机中存储的数据都是01二进制的数据串，我们再电脑屏幕上看到的一切可视化的东西最终在计算机存储的都是01二进制串，现在看到我博客上的文字也是。 这里就需要一个映射关系，将我们看到的看到的字符、图片、视频转换成对应的二进制，存储到计算机中，以便于计算机能够识别。这就是我们要将的编码。 图片和视频首先会转换成字符流，然后按照编码转换成字节流，存储到计算机中。 举一个例子：在ASCLL码表中，字符’a‘对应的是97，二进制是01100001，如果我们在编辑器或者其他软件中输入字母a，按键盘a，那么输入法就会告诉操作系统，在计算机中调用97，然后就会显示a字符。 也就是说编码表在我们在计算机屏幕看到的数字和计算机底层之间架起了一个桥梁。至关重要！ 二、编码的历程1、开始 最早出现的是ASCLL码，是由美国人制定的，这种编码采用的是1个字节表示一个字符，也就是一个字符是八个二进制位，这里一共可以表示256个字符。起初，表示英文中所有的字符一共用了128位字符。事实上还剩下了一百多个字符空间。这128个字符中包含了数字、字母以及常用字符，例如：A是65，对应的二进制数为0100 0001。而且呢，这个128个字符只用了八位二进制数中的后七位，最前面的一位统一为0。 2、问题初现 但是很快就暴露出了问题，这128个字符表示英文没有任何问题，但是要是表示其他国家的语言就不够了。率先是欧洲决定利用起来前一位，这样就可以表示256个字符了，而紧接着又出现了一个问题，就是不同国家的同一个码点（八位二进制数）表示的符号可能不同，例如144 在阿拉伯人的 ASCII 码中是 گ，而在俄罗斯的 ASCII 码中是 ђ。 所以呢，最后人们在0-127号字符上面达成了一致，对于128 - 255号字符就尽情发挥了，这时候的码点还是一个字节。 3、再遇到波折 随着计算机的兴起，传入亚洲国家，亚洲国家有更多的字符需要存储，这样256个字符就放不下这些符号了，这时候人们决定用两个字节一共十六位，最多可以表示65535个字符。来存储全世界的字符。但是此时各个国家的编码还是不一样的。如ASCII、GB2312等。 4、程序员的噩梦 各种各样的编码方式成了系统开发者的噩梦，因为他们想把软件卖到国外，但是由于编码不同，一台计算机根本无法表示所有国家的编码。于是，他们提出了一个“内码表”的概念，可以切换到相应语言的一个内码表，这样才能显示相应语言的字母。在这种情况下，如果使用多语种，那么就需要频繁的在内码表内进行切换。这样依然很麻烦。 5、解决 最终，美国人决定设计一种标准方案来展示世界上所有语言中的所有字符，Unicode诞生了。Unicode 给所有的字符指定了一个数字用来表示该字符。规定了符合对应的二进制代码，至于这个二进制代码如何存储则没有任何规定。它的想法很简单，就是为每个字符规定一个用来表示该字符的数字，仅此而已。具体是怎样的对应关系，又或者说是如何进行划分的，就不是我们考虑的问题了， 三、Unicode 编码方案 Unicode 当然是一本很厚的字典，记录着世界上所有字符对应的一个数字。以汉字“汉”为例，它的 Unicode 码点是 0x6c49，对应的二进制数是 110110001001001，二进制数有 15 位，这也就说明了它至少需要 2 个字节来表示。可以想象，在 Unicode 字典中往后的字符可能就需要 3 个字节或者 4 个字节。 1、空间浪费？ 大家可以试想一下，有的字符需要两个字节、有的需要三个四个字节，那计算机如何去分辨该字节是几个字节的字符呢？大家第一个想到的可能是全部采用4个字节的编码，毕竟这个可以表示上亿个字符了。但是如果一个美国的新闻发布网站，全英文表示（前面已经说了代码单元也就是存储为一个字节就可以表示所有的英文字符），现在前面三个字节都是0，文件大小变成了之前的四倍或者三倍，这样浪费掉了大量的存储空间。 2、UTF系列编码什么鬼？ 为了较好的解决 Unicode 的编码问题， UTF-8 和 UTF-16 两种当前比较流行的编码方式诞生了。当然还有一个 UTF-32 的编码方式，也就是上述那种定长编码，字符统一使用 4 个字节，但是一般的字符两个字节就可以轻松解决了，只有一些其他小国家的特殊字符才采用UTF-32编码。 3、UTF-8编码 UTF-8 是一个非常惊艳的编码方式，漂亮的实现了对 ASCII 码的向后兼容，以保证 Unicode 可以被大众接受。 UTF-8 是目前互联网上使用最广泛的一种 Unicode 编码方式，它的最大特点就是可变长。它可以使用 1 - 4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下： 对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码点。因此，对于英文中的 0 - 127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码那个年代的文档用 UTF-8 编码打开完全没有问题。 对于需要使用 N 个字节来表示的字符（N &gt; 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为0，剩余的 N - 1 个字节的前两位都设位 10，剩下的二进制位则使用这个字符的 Unicode 码点来填充。 编码规则如下： Unicode 十六进制码点范围 UTF-8 二进制 0000 0000 - 0000 007F 0xxxxxxx 0000 0080 - 0000 07FF 110xxxxx 10xxxxxx 0000 0800 - 0000 FFFF 1110xxxx 10xxxxxx 10xxxxxx 0001 0000 - 0010 FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 4、插入一只广告 这里的除x外都是控制位，真正的表示位的只是x，第一个字节0xxxxxxx，可以容纳7个二进制位，第二个可以容纳11个二进制位，以此类推，然后计算左边的范围。 5、编码与解码 根据上面编码规则对照表，进行 UTF-8 编码和解码就简单多了。下面以汉字“汉”为利，具体说明如何进行 UTF-8 编码和解码。“汉”的 Unicode 码点是 0x6c49（110 1100 0100 1001），通过上面的对照表可以发现，0x0000 6c49 位于第三行的范围，那么得出其格式为 1110xxxx 10xxxxxx 10xxxxxx。接着，从“汉”的二进制数最后一位开始，从后向前依次填充对应格式中的 x，多出的 x 用 0 补上。这样，就得到了“汉”的 UTF-8 编码为 11100110 10110001 10001001，转换成十六进制就是 0xE6 0xB7 0x89。 解码的过程也十分简单：如果一个字节的第一位是 0 ，则说明这个字节对应一个字符；如果一个字节的第一位1，那么连续有多少个 1，就表示该字符占用多少个字节。 UTF-16编码 在了解 UTF-16 编码方式之前，先了解一下另外一个概念——“平面”。 在上面的介绍中，提到了 Unicode 是一本很厚的字典，她将全世界所有的字符定义在一个集合里。这么多的字符不是一次性定义的，而是分区定义。每个区可以存放 65536 个（2^16）字符，称为一个平面（plane）。目前，一共有 17 个（2^5）平面，也就是说，整个 Unicode 字符集的大小现在是 2^21。 最前面的 65536 个字符位，称为基本平面（简称 BMP ），它的码点范围是从 0 到 2^16-1，写成 16 进制就是从 U+0000 到 U+FFFF。所有最常见的字符都放在这个平面，这是 Unicode 最先定义和公布的一个平面。剩下的字符都放在辅助平面（简称 SMP ），码点范围从 U+010000 到 U+10FFFF。 基本了解了平面的概念后，再说回到 UTF-16。UTF-16 编码介于 UTF-32 与 UTF-8 之间，同时结合了定长和变长两种编码方法的特点。它的编码规则很简单：基本平面的字符占用 2 个字节，辅助平面的字符占用 4 个字节。也就是说，UTF-16 的编码长度要么是 2 个字节（U+0000 到 U+FFFF），要么是 4 个字节（U+010000 到 U+10FFFF）。那么问题来了，当我们遇到两个字节时，到底是把这两个字节当作一个字符还是与后面的两个字节一起当作一个字符呢？ 这里有一个很巧妙的地方，在基本平面内，从 U+D800 到 U+DFFF 是一个空段，即这些码点不对应任何字符。因此，这个空段可以用来映射辅助平面的字符。 辅助平面的字符位共有 2^20 个，因此表示这些字符至少需要 20 个二进制位。UTF-16 将这 20 个二进制位分成两半，前 10 位映射在 U+D800 到 U+DBFF，称为高位（H），后 10 位映射在 U+DC00 到 U+DFFF，称为低位（L）。这意味着，一个辅助平面的字符，被拆成两个基本平面的字符表示。 因此，当我们遇到两个字节，发现它的码点在 U+D800 到 U+DBFF 之间，就可以断定，紧跟在后面的两个字节的码点，应该在 U+DC00 到 U+DFFF 之间，这四个字节必须放在一起解读。 接下来，以汉字”𠮷”为例，说明 UTF-16 编码方式是如何工作的。 汉字”𠮷”的 Unicode 码点为 0x20BB7，该码点显然超出了基本平面的范围（0x0000 - 0xFFFF），因此需要使用四个字节表示。首先用 0x20BB7 - 0x10000 计算出超出的部分，然后将其用 20 个二进制位表示（不足前面补 0 ），结果为0001000010 1110110111。接着，将前 10 位映射到 U+D800 到 U+DBFF 之间，后 10 位映射到 U+DC00 到 U+DFFF 即可。U+D800 对应的二进制数为 1101100000000000，直接填充后面的 10 个二进制位即可，得到 1101100001000010，转成 16 进制数则为 0xD842。同理可得，低位为 0xDFB7。因此得出汉字”𠮷”的 UTF-16 编码为 0xD842 0xDFB7。 Unicode3.0 中给出了辅助平面字符的转换公式： 123H = Math.floor((c-0x10000) / 0x400)+0xD800L = (c - 0x10000) % 0x400 + 0xDC00 根据编码公式，可以很方便的计算出字符的 UTF-16 编码。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>unicode编码</tag>
        <tag>uft-8编码</tag>
        <tag>uft-16编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于浮点数丢失精度的原理]]></title>
    <url>%2F2019%2F07%2F29%2F%E5%85%B3%E4%BA%8E%E6%B5%AE%E7%82%B9%E6%95%B0%E4%B8%A2%E5%A4%B1%E7%B2%BE%E5%BA%A6%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[浮点数会存在精度丢失的现象，大很多金融的大项目中，都不去使用浮点数。因为这个数很危险，我们可以剖析一下原理。 1、前言首先我们必须要清楚 在计算机中所有的数值、代码、信息都是以01二进制存储的，也就是说我们输入的所有信息，最终都会表示成01二进制的形式。例如byte类型的 0000 1000表示的是整数8。然后我们要清楚 所有的整数类型转换成二进制，看如下代码： 12345//表示11的二进制数计算11 / 2 = 5 余 1 --&gt; 15 / 2 = 2 余 1 --&gt; 12 / 2 = 1 余 0 --&gt; 0 1 / 2 = 0 余 1 --&gt; 1 所有的小数转换成二进制：看如下代码： 1234567//0.1转换为二进制0.1 * 2 = 0.2 --&gt;00.2 * 2 = 0.4 --&gt;00.4 * 2 = 0.8 --&gt;00.8 * 6 = 1.6 --&gt;1//然后去掉10.6 * 2 = 1.2 --&gt;10.2 * 2 = 0.4 --&gt;0 这个是一直循环下去的，所以呢计算机无法精确表示0.1，这个数值而是无限接近近似表示。 所以11的二进制数是1011，这样开来所有的整数最后都会被1 / 2 = 0这样结束。 2、我们先来看这样的一段代码12345double dou = 234533464.456576564675d;System.out.println("234533464.456576564675的运行结果："+dou);float f = 997979759f;System.out.println("997979759的运行结果："+f);System.out.println("0.1+0.2的运行结果："+(0.2 + 0.1)); 结果是： 惊奇不惊奇，刺激不刺激？这种采用浮点数的方法，输出的结果就是错误的。前两位后面的精度丢失，最后一个表示的不准确。 同样再看另外一段代码123456float f1 = 20014999; double d1 = f1; double d2 = 20014999; System.out.println("f=" + f1); System.out.println("d=" + d1); System.out.println("d2=" + d2); 运行结果： 又是满满的疑问123double a = 0.3d;System.out.println(0.2d+0.1d);System.out.println(a); 我估计第三个直接刷新了好多人的三观，为啥直接存储就输出正确，运算就出错了呢？接下来我就围绕这几个问题对对大家一一解答。3、我们先了解一下浮点数在计算机中是如何存储的。 float（32位浮点数）在计算机中的表示形式。 浮点数存储都遵守IEEE754标准，具体的运算应该在计算机组成原理中会重点介绍，标准如下： 其中S表示该浮点数的正负，E代表阶码，M代表的是尾数。 一个十进制数可以表示为 : 例如：20.59375在计算机中存储为： 20.59375 = 10100.10011，这里的s = 0，E = 4 + 127 = 131 ，M = 01001001； 所以存储格式为0100 001 1010 0100 1100 0000 0000 0000。这里就是存储在计算机中的数据。 至于为什么要加127，这个是IEEE754标准规定的，但是在维基百科以及其他文献中也并没有直接说为什么是这样。 double（64位浮点数）在计算机中的表示形式。 同理这个和上面的是一样的。 实际上这个1.M表示的是比如1000.111这个数，小数点移动位为1.000111，在754标准下存储为000111位数，这么做相当于是能够多保存一位，所以float可以保存的尾数是24位（23），double为53位而不是（52）。4、我们首先来解决第二个问题 123float f1 = 20014999; double d1 = f; double d2 = 20014999; 刚才不是说所有的整数都不会丢失精度吗，这个这么不一样呢。这个实际上也是因为float保留的位数太小造成的。 我们来分析一下：先输出他们的二进制位数 123456789float f1 = 20014999; double d1 = f1; double d2 = 20014999;long l1 = Float.floatToIntBits(f1);long l2 = Double.doubleToLongBits(d1);long l3 = Double.doubleToLongBits(d2);System.out.println("f1=" + Long.toBinaryString(l1)); System.out.println("d1=" + Long.toBinaryString(l2)); System.out.println("d2=" + Long.toBinaryString(l3)); 这里遵循IEEE754标准，但是注意这里面没有符号位，这三种结果为什么不同，我们这就分析，首先我告诉大家d2的输出结果是正确的。其中前11位是阶码1000 0010 111= 1047，后面的0011 0001 0110 0111 1001 0111 0000 0000 0000 0000 0000 0000 0000便是尾数，所以1047 - 1023 = 24，所以这个数真正的结果就是（别忘了1.M）：1.0011 0001 0110 0111 1001 0111然后小数点向右移动24位，就是10011 0001 0110 0111 1001 0111 = 20014999； 我们再来看第一个我们对比着尾数1.00110001011001111001100就会发现这里的尾数要比1.001100010110011110010111少了一位，而且少的一位是1，所以进行舍入处理，进行进位变成了1.00110001011001111001100。这样就产生了误差。也就变成了20015000。大家要清楚这里不光是尾数的位数少了还有相应的进位处理。同时要记得这里面float只能保存24位小数，double可以保留53位。 第二个就不用说了，由于本身f是错的，所以呢，赋值给d1之后仍然是是错的。 第三个由于54尾尾数可以放得下该数值的二进制数，所以是正确的。 到此为止呢，大家要明确一个概念就是，如果保存的浮点数超过了，该类型的最大精度，那么就会产生是很大很严重的问题，而且存入计算机中就会是存储的错的。明确这一点之后也就产生了我们第三个问题（非常奇怪的问题），所以呢我们最后来说这个问题。5、解决第一个问题。 讲过第二个问题之后，第一个问题就非常好理解了，我们通过IEEE754的算法，可以得到这两个数的二进制表示形式，当用754标准去选取尾数的时候呢，就会截取掉一部分的尾数，造成精度丢失，其实原理是一样的。234533464.456576564675 = 2.34533464456576564675*e8,然后仍然是转换成754标准就失去了一些精度。具体的算法需要朋友们找相关的资料，这里不在赘述。6、最奇怪的问题第三个问题。1234567double a = 0.1d;double b = 0.2d;double c = a + b;System.out.println(a+&quot;的二进制数:&quot;+Long.toBinaryString(Double.doubleToLongBits(a)));System.out.println(b+&quot;的二进制数:&quot;+Long.toBinaryString(Double.doubleToLongBits(b)));System.out.println(c+&quot;的二进制数:&quot;+Long.toBinaryString(Double.doubleToLongBits(c)));System.out.println(0.3+&quot; 的二进制数:&quot;+Long.toBinaryString(Double.doubleToLongBits(0.3d))); 大家可以看到直接存储的0.3和计算之后出现的（数学上来说的0.3）在计算机中保存的二进制编码是不同的，原因就在于首先计算机中无法精确表示0.1和0.2，所以实际上a，b并不是真正的0.1和0.2，已经出现了误差，所以相加之后计算出来的值就是错的，也就是0.30000000000000004。那至于为什么能够出现直接存储就可以正常显示呢，是由于编译器优化的结果，在不计算的情况下，可以正常显示，但是没有任何意义，因为一旦参与计算或者比较大小，那么这个值就不代表数学意义上的0.1了。]]></content>
      <categories>
        <category>java基础知识</category>
      </categories>
      <tags>
        <tag>浮点数精度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript预编译（执行期上下文）]]></title>
    <url>%2F2019%2F07%2F28%2FJavaScript%E9%A2%84%E7%BC%96%E8%AF%91%EF%BC%88%E6%89%A7%E8%A1%8C%E6%9C%9F%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[很多人前端人员都知道变量提升这个概念，但是为什么会出现这样，很多人模棱两可。实际上在js中这叫做预编译，也叫做执行期上下文，在这里深度剖析一下它的底层原理。 预编译（又叫“执行期上下文”）一般来说，预编译与闭包、作用域链和闭包是密切相关的，好多人包括在企业开发的人都听说过一句话叫做变量提升和“函数声明整体提升”。这个就是预编译后的一个小结果，所以今天自己总结了一下。分为以下几个步骤来说。 大家先看看这段代码12console.log(a);var a = 1; 为什么变量的声明在输出的下面，却没有报错？值为什么是undefined？ 大家再看看下面这段代码1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script&gt; function test(a,b)&#123; console.log(a); var a = 'demo'; console.log(a); function a()&#123;&#125; console.log(a); var a = function ()&#123;&#125; console.log(a); console.log(b) var b = 1; &#125; test(1);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 大家看到这段代码是不是很震惊呢？形参、变量名、函数名都是“a”，那么输出的结果会是怎样呢？ 和大家想的一样吗？大家心里是不是有这么几个问题，前四个输出的都是a，为什么输出的结果不一样呢？而且b变量不是把1赋值给它了吗，为什么会是undefined？接下来咱们来讲讲什么是预编译（执行期上下文）。到最后大家就明白了。 预编译（‘执行期上下文’）预编译发生在函数执行之前。划重点了啊！！！！！ 这句话很重要，函数执行之前也就是在这段程序开始之前，浏览器对马上要执行的函数进行预编译！！预编译四部曲 创建AO对象 找形参和变量声明，将变量和形参作为AO属性名，值为undefined 将实参和形参相统一 在函数体里找到函数声明，值赋予函数体最后程序输出变量值的时候，就是从AO对象中拿。]]></content>
      <categories>
        <category>Web前端开发</category>
      </categories>
      <tags>
        <tag>变量提升</tag>
        <tag>预编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F27%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>tags1</tag>
        <tag>tags2</tag>
      </tags>
  </entry>
</search>
